{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Khoa học dữ liệu**  \n",
    "Học kì I, 2020 - 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><b><font size=\"10\">ĐỒ ÁN CUỐI KÌ</font></b></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=\"6\">Dự đoán giá cổ phiếu</font></b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Nhóm thực hiện:</b><br>Trần Minh Trí - 1712834<br>Nguyễn Nhật Trường - 1712852</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dự đoán giá cổ phiếu dựa theo pattern giá trong quá khứ.\n",
    "* Trả lời được câu hỏi trên sẽ giúp người chơi cổ phiếu quyết định mua hay bán một loại cổ phiếu nào đó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Các thư viện hỗ trợ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.robotparser\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from requests_html import HTML\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import statsmodels.api as sm\n",
    "import os.path\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as backend\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import callbacks\n",
    "import pickle\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "import gc\n",
    "\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Thu thập dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nguồn dữ liệu**: Trang web [CafeF](https://s.cafef.vn/) là nguồn sử dụng để thu thập dữ liệu. Từng loại cổ phiếu được sử dụng sẽ được thu thập thông qua trang tìm kiếm của cổ phiếu đó, cụ thể: [BHV](https://s.cafef.vn/Lich-su-giao-dich-BVH-1.chn?fbclid=IwAR0e98txe3qOw8SP_cTAVxXqeTN2CnuAiOnnLMzUXovyH-zJRZXVNBWU2sg).  \n",
    "Dữ liệu được thu thập bằng phương thức parse HTML sử dụng `selenium`, đến ngày 17/12/2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu đã được kiểm tra có thể thu thập:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url('https://s.cafef.vn/robots.txt')\n",
    "rp.read()\n",
    "rp.can_fetch('*','https://s.cafef.vn/Lich-su-giao-dich-BVH-1.chn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_id = []\n",
    "for i in range(1, 21):\n",
    "    if i%2: row_id.append(str(i).zfill(2) + '_')\n",
    "    else: row_id.append(str(i).zfill(2) + '_alt')\n",
    "\n",
    "def get_stock_data(stock_symbol, output_file):\n",
    "    url = 'https://s.cafef.vn/Lich-su-giao-dich-' + stock_symbol + '-1.chn'    \n",
    "    \n",
    "    file = open(output_file, 'w', encoding='utf-8')\n",
    "    file.write(f'Date,Open,High,Low,Close\\n')\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path='./chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    html = HTML(html=driver.page_source)\n",
    "\n",
    "    while True:        \n",
    "        for i in row_id:\n",
    "            html = HTML(html=driver.page_source)\n",
    "            row = html.find('tr#ctl00_ContentPlaceHolder1_ctl03_rptData2_ctl' + i + 'itemTR', first=True)\n",
    "            if row:                \n",
    "                date = row.find('td.Item_DateItem', first=True).text\n",
    "                date = pd.to_datetime(date, format='%d/%m/%Y').strftime('%Y-%m-%d')\n",
    "                \n",
    "                prices = row.find('td.Item_Price10')\n",
    "                op, hi, lo, cl = prices[5].text, prices[6].text, prices[7].text, prices[1].text\n",
    "                file.write(f'{date},{op},{hi},{lo},{cl}\\n')\n",
    "\n",
    "        button = driver.find_elements(By.LINK_TEXT, '>')\n",
    "        if len(button) > 0:\n",
    "            button[0].click()\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('csv/BVH.csv'):\n",
    "    get_stock_data('BVH', 'csv/BVH.csv')\n",
    "else:\n",
    "    print('File', 'csv/BVH.csv', 'existed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Phân tích và tiền xử lí dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BVH = pd.read_csv('csv/BVH.csv', parse_dates={'Datetime':['Date']}).iloc[::-1]\n",
    "BVH.set_index(['Datetime'], inplace = True)\n",
    "BVH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Có 5 cột:\n",
    "  * `Date`: ngày.\n",
    "  * `Open`: giá mở cửa.\n",
    "  * `High`: giá cao nhất.\n",
    "  * `Low`: giá thấp nhất.\n",
    "  * `Close`: giá đóng cửa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BVH.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dữ liệu (chưa tiền xử lý), sau khi đưa cột `Date` thành index, có 2868 dòng và 4 cột, với cả 4 cột đều có kiểu dữ liệu `float`.\n",
    "* Ở đồ án này chỉ sử dụng cột `Close` cho mô hình hóa và dự đoán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Closing price since {0} to {1}'.format(BVH.index[0].strftime(\"%d/%m/%Y\"), \n",
    "                                                  BVH.index[-1].strftime(\"%d/%m/%Y\")), fontsize=18)\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Closing Price', fontsize=18)\n",
    "plt.plot(BVH['Close'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phân tích thành phần chuỗi thời gian (**Time-series decomposition**) cho dãy giá đóng. Quá trình này cho phép chuỗi thời gian được thể hiện qua 3 đặc trưng chính là **Trend**, **Seasonality** và **Noise**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 18, 8\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(BVH['Close'], model = 'additive', period=7)\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('BVH Close Price Decomposition', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta có nhận xét: chuỗi thời gian hoàn toàn không có tính **seasonality** hay có xu hướng - **trend** nào rõ rệt. Hơn nữa, xét về **noise**, dữ liệu mang tính khá ngẫu nhiên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lí dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vấn đề sau khi thu thập**: Dữ liệu thu thập được là các dòng theo từng ngày, tuy nhiên, có một số ngày bị thiếu do website không cập nhật giá cổ phiếu vào các ngày đó. Ta cần phải tiền xử lý dữ liệu để \"fill\" các giá trị còn thiếu ở các ngày đó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thêm những điểm dữ liệu thiếu bằng đoạn giữa với 2 đầu là 2 điểm quan sát được. (VD: [1, nan, nan, 7] -> [1, 3, 5, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_time_point(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    i = 0\n",
    "    curr = df.index[0]\n",
    "    end = df.index[-1]\n",
    "\n",
    "    while curr != end:\n",
    "        curr += datetime.timedelta(days=1)\n",
    "        i+=1\n",
    "        if curr != df.index[i]:\n",
    "            before = df.iloc[i-1].to_numpy().copy()\n",
    "            after = df.iloc[i].to_numpy().copy()\n",
    "            days_missed = (df.index[i] - curr).days\n",
    "            avg = ((after - before)/(days_missed+1)).round(1)\n",
    "\n",
    "            while curr != df.index[i]:\n",
    "                before += avg\n",
    "                fix = pd.DataFrame({'Open': before[0], 'High': before[1], 'Low': before[2], 'Close': before[3]}, index=[curr])\n",
    "                df = df.append(fix)\n",
    "                #print(fix)\n",
    "                curr += datetime.timedelta(days=1)\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BVH_filled = fill_time_point(BVH)\n",
    "BVH.shape, BVH_filled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi tiền xử lý, dữ liệu mới gồm có 4193 dòng và 4 cột:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BVH_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biểu đồ thể hiện những điểm được điền:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "available = BVH.index\n",
    "date_set = set(available[0] + timedelta(x) for x in range((available[-1] - available[0]).days))\n",
    "missing = sorted(date_set - set(available))\n",
    "\n",
    "available_df = BVH.reindex(date_set).sort_index()\n",
    "missing_df = BVH_filled.loc[missing].reindex(date_set).sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "ax1 = available_df['Close'].plot(label='Available data')\n",
    "ax1.legend(fontsize=12)\n",
    "\n",
    "ax2 = missing_df['Close'].plot(color='red', label='Missing data')\n",
    "ax2.legend(fontsize=12)\n",
    "\n",
    "fig.suptitle('BVH Close Price', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "ax1 = available_df['Close'][-365:].plot(marker='.', label='Available data')\n",
    "ax1.legend(fontsize=12)\n",
    "\n",
    "ax2 = missing_df['Close'][-365:].plot(color='red', marker='.', label='Missing data')\n",
    "ax2.legend(fontsize=12)\n",
    "\n",
    "fig.suptitle('BVH Close Price in the last 365 days', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình dự đoán giá đóng cửa trong ngày, cần tách riêng ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_close_price(df):\n",
    "    return df['Close'].values[:, np.newaxis] # reshape cho input của scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tách dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu ban đầu được chia thành 3 tập: train, validation, test với tỉ lệ (gần đúng) 70% - 15% - 15%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train - validation - test\n",
    "#        70%        15%       15%\n",
    "def split_data(dataset):\n",
    "    train_data, test_data = train_test_split(dataset, shuffle=False, test_size=0.15)\n",
    "    train_data, validation_data = train_test_split(train_data, shuffle=False, test_size=0.177)\n",
    "    \n",
    "    return train_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuẩn hóa dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhóm sử dụng MinMaxScaler để chuẩn hóa dữ liệu về `range(0, 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuyển dữ liệu về các timestep sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tập X (input) chứa dữ liệu giá đóng cửa *step* ngày trước ngày *n* cần dự đoán.\n",
    "* Tập Y (output) chứa dữ liệu giá đóng cửa 1 ngày *n* cần dự đoán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timestep_Converter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, steps=50, lag=0):\n",
    "        self.steps = steps\n",
    "        self.lag = lag\n",
    "    def fit(self, X_df, y=None):\n",
    "        return self\n",
    "    def transform(self, data, y=None):\n",
    "            X = []\n",
    "            Y = []\n",
    "\n",
    "            for i in range(len(data)):\n",
    "                end_ixd = i + self.steps\n",
    "\n",
    "                if end_ixd > len(data)-1:\n",
    "                    break\n",
    "\n",
    "                seq_x, seq_y = data[i:end_ixd-self.lag], data[end_ixd]\n",
    "                X.append(np.array(seq_x))\n",
    "                Y.append(seq_y)\n",
    "\n",
    "            X = np.array(X).reshape(len(X), self.steps-self.lag, 1)\n",
    "            Y = np.array(Y)\n",
    "\n",
    "            return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mô hình hóa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Tạo pipeline tiền xử lí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = make_pipeline(FunctionTransformer(fill_time_point),\n",
    "                                    FunctionTransformer(get_close_price),\n",
    "                                    MinMaxScaler(feature_range=(0,1)),\n",
    "                                    Timestep_Converter())\n",
    "\n",
    "preprocess_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Xây dựng mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhóm sử dụng **mô hình LSTM** của thư viện `keras`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM(input_shape, lr):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True,input_shape=input_shape))\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(units=50))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timeseries_Model:\n",
    "    def __init__(self, steps=50, lr=0.001, batch_size=1, patience=15, preprocess_pipeline=None):\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.preprocess_pipeline = preprocess_pipeline\n",
    "        if preprocess_pipeline is not None:\n",
    "            self.preprocess_pipeline.set_params(timestep_converter__steps = self.steps)\n",
    "        \n",
    "    def set_params(self, **kwargs):\n",
    "        self.steps = kwargs.get('steps', self.steps)\n",
    "        self.preprocess_pipeline.set_params(timestep_converter__steps = self.steps)\n",
    "        self.lr = kwargs.get('lr', self.lr)\n",
    "        self.batch_size = kwargs.get('batch_size', self.batch_size)\n",
    "        self.patience = kwargs.get('patience', self.patience)\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.model = create_LSTM((self.steps,1), self.lr)\n",
    "\n",
    "    def fit(self, train_data, validation_data=None):\n",
    "        X_train, Y_train = self.preprocess_pipeline.fit_transform(train_data)\n",
    "        if validation_data is not None:\n",
    "            X_val, Y_val = self.preprocess_pipeline.transform(validation_data)\n",
    "        \n",
    "            earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", mode =\"min\", \n",
    "                                                    patience = self.patience, \n",
    "                                                    restore_best_weights = True, verbose=2)\n",
    "        \n",
    "            return self.model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                            batch_size=self.batch_size, verbose=0, epochs=100, \n",
    "                                  callbacks=[earlystopping])\n",
    "        else:\n",
    "            earlystopping = callbacks.EarlyStopping(monitor =\"loss\", mode =\"min\", \n",
    "                                                    patience = self.patience, \n",
    "                                                    restore_best_weights = True, verbose=2)\n",
    "            return self.model.fit(X_train, Y_train, \n",
    "                            batch_size=self.batch_size, verbose=0, epochs=100, \n",
    "                                  callbacks=[earlystopping])\n",
    "    \n",
    "    def predict(self, data, return_y = False):\n",
    "        X, Y = self.preprocess_pipeline.transform(data)\n",
    "        scaler = self.preprocess_pipeline['minmaxscaler']\n",
    "\n",
    "        if return_y:\n",
    "            return scaler.inverse_transform(Y), scaler.inverse_transform(self.model.predict(X))\n",
    "        return scaler.inverse_transform(self.model.predict(X))\n",
    "    \n",
    "    def error(self, data):\n",
    "        X, Y = self.preprocess_pipeline.transform(data)\n",
    "        scaler = self.preprocess_pipeline['minmaxscaler']\n",
    "        \n",
    "        Y_pred = scaler.inverse_transform(self.model.predict(X))\n",
    "        Y_ = scaler.inverse_transform(Y)\n",
    "\n",
    "        return np.mean( (Y_pred-Y_)**2 )\n",
    "    \n",
    "    def save(self, file):\n",
    "        self.model.save(file)\n",
    "        \n",
    "    def load(self, file):\n",
    "        if not os.path.exists(to_save):\n",
    "            print(\"File\", file, \"doesnt exist\")\n",
    "            return\n",
    "        self.model = load_model(file)\n",
    "        \n",
    "    def get_scaler(self):\n",
    "        return self.preprocess_pipeline['minmaxscaler']\n",
    "    \n",
    "    def predict_ahead(self, data, number_of_date):\n",
    "        X, Y = self.preprocess_pipeline.transform(data)\n",
    "        scaler = self.preprocess_pipeline['minmaxscaler']\n",
    "        \n",
    "        last_X = X[-1][:,0]\n",
    "        prediction = np.array([])\n",
    "\n",
    "        for i in range(number_of_date):\n",
    "            next_Y = self.model.predict(last_X.reshape(1, len(last_X), 1))\n",
    "            prediction = np.append(prediction, next_Y[0][0])\n",
    "            last_X = np.append(last_X[1:], next_Y)\n",
    "\n",
    "        prediction = scaler.inverse_transform(prediction.reshape(-1, 1))\n",
    "\n",
    "        idx = pd.date_range(data.index[-1], periods=number_of_date+1, freq='1d')[1:]\n",
    "        return pd.Series(prediction[:,0], index=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Tìm mô hình tốt nhất"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhóm thử nghiệm các tham số `step`, `learning rate` và `batch_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = split_data(BVH)\n",
    "preprocess_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Warning: Lâu\n",
    "total_t = time.time()\n",
    "list_steps=[30, 40, 50, 60]\n",
    "list_lr=[0.001, 0.01, 0.1, 1]\n",
    "list_batch_size=[16, 32, 64, 128]\n",
    "\n",
    "val_errs = dict()\n",
    "best_val_err = 999999\n",
    "best_steps, best_lr, best_batch_size = 0,0,0\n",
    "\n",
    "for steps in list_steps:\n",
    "\n",
    "    val_err = []\n",
    "    for lr in list_lr:\n",
    "        err = []\n",
    "        \n",
    "        for batch_size in list_batch_size:\n",
    "            model = Timeseries_Model(steps = steps, lr = lr, batch_size = batch_size, \n",
    "                                     preprocess_pipeline=preprocess_pipeline)\n",
    "            \n",
    "            to_save = \"models/LLDrDD_lr={0}_step={1}_batchsize={2}\".format(lr, steps, batch_size)\n",
    "            starttime = time.time()\n",
    "            print(\"<<<< Started\", to_save, \"at\", \n",
    "                  time.strftime('%H:%M:%S', time.localtime(starttime)), '>>>>')\n",
    "        \n",
    "            if not os.path.exists(to_save):\n",
    "                print('     Building model')\n",
    "                model.build_model()\n",
    "                history = model.fit(train_data, validation_data)\n",
    "                model.save(to_save)\n",
    "            else:\n",
    "                print('     Loading model')\n",
    "                model.load(to_save)\n",
    "            print(\"<<<< Model builed/loaded. Time elapsed:\", time.time() - starttime,'>>>>\\n')\n",
    "            \n",
    "            mse = model.error(validation_data)\n",
    "            \n",
    "            \n",
    "            if mse < best_val_err:\n",
    "                best_val_err = mse\n",
    "                best_steps, best_lr, best_batch_size = steps, lr, batch_size\n",
    "                \n",
    "            err.append(round(mse, 3))\n",
    "            \n",
    "            # Free memory            \n",
    "            del model\n",
    "            backend.clear_session() \n",
    "            gc.collect()\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        val_err.append(err)\n",
    "\n",
    "    val_errs[steps] = pd.DataFrame(val_err,columns=list_batch_size,index=list_lr)\n",
    "\n",
    "print(\"Finished! Total time elapsed:\", time.time() - total_t, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả các model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in val_errs.keys():\n",
    "    display(val_errs[k])\n",
    "    print(\"Steps =\", k, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "pickle.dump(val_errs, open('result.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tốt nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_err, best_steps, best_lr, best_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = Timeseries_Model(steps = best_steps, lr = best_lr, batch_size = best_batch_size, \n",
    "                              preprocess_pipeline=preprocess_pipeline)\n",
    "\n",
    "best = \"models/LLDrDD_lr={0}_step={1}_batchsize={2}\".format(best_lr, best_steps, best_batch_size)\n",
    "best_model.load(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kết quả model tốt nhất trên tập validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_prediction(Y, predicted_Y, idx, title='', eval_error=False, zoom_in=[], zoom_title=''):\n",
    "    valid_vs_prediction = pd.DataFrame({'Actual': Y.reshape(len(Y))}, index=idx)\n",
    "    valid_vs_prediction['Predicted'] = predicted_Y.reshape(len(predicted_Y))\n",
    "    valid_vs_prediction['Error'] = valid_vs_prediction['Actual'] - valid_vs_prediction['Predicted']\n",
    "    \n",
    "    std_part = 1.96 # Độ tin cậy 95%, [11]\n",
    "    err_std = valid_vs_prediction['Error'].std(axis=0)\n",
    "    err_mean = np.absolute(valid_vs_prediction['Error'].mean(axis=0))\n",
    "\n",
    "    pred_upper = valid_vs_prediction['Predicted'] + err_mean + err_std * std_part\n",
    "    pred_lower = valid_vs_prediction['Predicted'] - err_mean - err_std * std_part\n",
    "    \n",
    "    if eval_error:        \n",
    "        print('MSE:', np.mean( (valid_vs_prediction['Actual'] - valid_vs_prediction['Predicted'])**2 ))\n",
    "        print(valid_vs_prediction['Error'].describe())\n",
    "        plt.figure(figsize=(8,4))\n",
    "        valid_vs_prediction['Error'].hist(bins=50).set_title('Error distribution')\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(8,4))\n",
    "        valid_vs_prediction['Error'].plot(kind='box', grid=True).set_title(\"Error\")\n",
    "        plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date', fontsize=18)\n",
    "    plt.ylabel('Closing Price', fontsize=18)\n",
    "    plt.plot(valid_vs_prediction['Actual'], color='b')\n",
    "    plt.plot(valid_vs_prediction['Predicted'], color='r')\n",
    "    plt.fill_between(x=valid_vs_prediction.index, y1=pred_upper, y2=pred_lower, \n",
    "                     color='green', lw=2, alpha=0.2)\n",
    "    plt.legend(['Actual', 'Predicted'], loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    if len(zoom_in) > 0:\n",
    "        plt.figure(figsize=(16,10))\n",
    "        plt.title(zoom_title)\n",
    "        plt.plot(valid_vs_prediction['Actual'][zoom_in[0]:zoom_in[1]], \n",
    "                 label='Actual', marker='o', color='b')\n",
    "        plt.plot(valid_vs_prediction['Predicted'][zoom_in[0]:zoom_in[1]], \n",
    "                 label='Predicted', marker='o', color='r')\n",
    "        plt.fill_between(x=valid_vs_prediction.index[zoom_in[0]:zoom_in[1]], \n",
    "                         y1=pred_upper[zoom_in[0]:zoom_in[1]], \n",
    "                         y2=pred_lower[zoom_in[0]:zoom_in[1]], color='green', lw=2, alpha=0.2)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_, Y_pred_ = best_model.predict(validation_data, return_y = True)\n",
    "plot_model_prediction(Y_, Y_pred_, fill_time_point(validation_data).index[best_steps:], \n",
    "                      title='Validation set', zoom_in=[-100, None], zoom_title='Last 100 days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4. Độ lỗi trên tập test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huấn luyện model với dữ liệu train và validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Timeseries_Model(steps = best_steps, lr = best_lr, batch_size = best_batch_size, \n",
    "                              preprocess_pipeline=preprocess_pipeline)\n",
    "final_model.build_model()\n",
    "final_model.fit(train_data.append(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_, Y_pred_ = final_model.predict(test_data, return_y = True)\n",
    "plot_model_prediction(Y_, Y_pred_, fill_time_point(test_data).index[best_steps:], \n",
    "                      title='Test set', eval_error=True, \n",
    "                      zoom_in=[-100, None], zoom_title='Last 100 days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dự đoán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhóm thử dự đoán giá đóng cửa cho 7 ngày tiếp theo, với mỗi ngày dự đoán được thêm vào chuỗi quan sát để dự đoán cho ngày tiếp theo:\n",
    "* Y(n) = f(O(n-1), O(n-2),...)\n",
    "* Y(n+1) = f(Y(n), O(n-1), O(n-2),...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_future_prediction(Y, predicted_Y, idx, next_predict, real_data, title='', show=0):\n",
    "    valid_vs_prediction = pd.DataFrame({'Actual': Y.reshape(len(Y))}, index=idx)\n",
    "    valid_vs_prediction['Predicted'] = predicted_Y.reshape(len(predicted_Y))\n",
    "    valid_vs_prediction['Error'] = valid_vs_prediction['Actual'] - valid_vs_prediction['Predicted']\n",
    "    \n",
    "    std_part = 1.96 # Độ tin cậy 95%, [11]\n",
    "    err_std = valid_vs_prediction['Error'].std(axis=0)\n",
    "    err_mean = np.absolute(valid_vs_prediction['Error'].mean(axis=0))\n",
    "\n",
    "    pred_upper = valid_vs_prediction['Predicted'][-show:] + err_mean + err_std * std_part\n",
    "    pred_lower = valid_vs_prediction['Predicted'][-show:] - err_mean - err_std * std_part\n",
    "    \n",
    "#     next_pred_upper, next_pred_lower = [], []\n",
    "#     for i in range(len(next_predict)):\n",
    "#         next_pred_upper.append(next_predict[i] + (err_mean + err_std * std_part*(i+1)))\n",
    "#         next_pred_lower.append(next_predict[i] - (err_mean + err_std * std_part*(i+1)))\n",
    "    \n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date', fontsize=18)\n",
    "    plt.ylabel('Close Price', fontsize=18)\n",
    "    plt.plot(valid_vs_prediction['Actual'][-show:], marker='o', color='b')\n",
    "    plt.plot(valid_vs_prediction['Predicted'][-show:], marker='o', color='r')\n",
    "    plt.plot(next_predict, marker='o', color='r')\n",
    "    plt.plot(real_data, marker='o', color='navy')\n",
    "    plt.fill_between(x=valid_vs_prediction.index[-show:], y1=pred_upper, y2=pred_lower, \n",
    "                     color='green', lw=2, alpha=0.2)\n",
    "#     plt.fill_between(x=next_predict.index, y1=next_pred_upper, y2=next_pred_lower, \n",
    "#                      color='green', lw=2, alpha=0.2)\n",
    "    plt.legend(['Actual', 'Prediction'], \n",
    "               loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = pd.read_csv('csv/BVH_next7.csv', parse_dates={'Datetime':['Date']}).iloc[::-1]\n",
    "read_data.set_index(['Datetime'], inplace = True)\n",
    "read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "next_predict = final_model.predict_ahead(test_data, number_of_date=7)\n",
    "Y_, Y_pred_ = final_model.predict(test_data, True)\n",
    "\n",
    "plot_future_prediction(Y_, Y_pred_, fill_time_point(test_data).index[best_steps:],\n",
    "                       next_predict, real_data=read_data['Close'], title='Next 7 days prediction', show=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Đánh giá quá trình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinh nghiệm tích lũy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Qua đồ án này, nhóm học được nhiều về mô hình hóa dự đoán cho kiểu dữ liệu chuỗi thời gian, kiểu dữ liệu chưa được demo trực tiếp trong khóa học\n",
    " - Học và làm quen với sử dụng pipeline cho việc huấn luyện được gọn gàng hơn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khó khăn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Lần đầu tiếp xúc với việc dự đoán trên kiểu timeseries nên nhóm phải tốn nhiều thời gian nghiên cứu, nhưng thấy được đó vẫn chưa đủ. Tiền xử lý, mô hình hóa còn nhiều chỗ có thể cải tiến để thu được mô hình tốt hơn\n",
    " - Cách dự đoán tương lai gần của mô hình có sai số qua các ngày tăng rất cao, hướng tăng trưởng khi đúng, khi sai tùy model. Nhóm xét thấy phần này thực hiện chưa thành công"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hướng phát triển nếu có thêm thời gian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Tìm hiểu thêm các phương pháp tiền xử lí để cải tiến mô hình\n",
    " - Tìm hiểu thêm về cách dự đoán tương lai gần khi đã có mô hình máy học\n",
    " - Tìm hiểu thêm và chạy thử các siêu tham số trên cái layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Nguồn tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://insights.magestore.com/posts/giai-thuat-time-series-forecasting\n",
    "\n",
    "[2] https://www.datacamp.com/community/tutorials/lstm-python-stock-market\n",
    "\n",
    "[3] https://randerson112358.medium.com/stock-price-prediction-using-python-machine-learning-e82a039ac2bb\n",
    "\n",
    "[4] https://viblo.asia/p/lam-quen-voi-keras-gGJ59mxJ5X2\n",
    "\n",
    "[5] https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "\n",
    "[6] https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "\n",
    "[7] https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "\n",
    "[8] https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "\n",
    "[9] https://viblo.asia/p/optimizer-hieu-sau-ve-cac-thuat-toan-toi-uu-gdsgdadam-Qbq5QQ9E5D8\n",
    "\n",
    "[10] https://www.phamduytung.com/blog/2018-10-02-understanding-epoch-batchsize-iterations/\n",
    "\n",
    "[11] https://otexts.com/fpp2/prediction-intervals.html\n",
    "\n",
    "[12] https://thorpham.github.io/blog/2018/05/25/keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "other-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
